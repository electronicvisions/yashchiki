@Library("jenlib") _

Closure cleanupSteps = {
	// NOTE: sudo commands have been manually permitted
	// remove sandboxes
	sh "sudo /bin/rm -rf \"${WORKSPACE}/sandboxes/\" || exit 0"
	// remove tmp spack
	sh "sudo /bin/rm -rf \"/tmp/${NODE_NAME}/\""
	// the spack repository gets bind mounted into the sandbox and owned by
	// spack user during build -> revert prior to cleaning worksapce
	sh "[ -d \"$WORKSPACE/spack\" ] && sudo chown -R vis_jenkins \"$WORKSPACE/spack\" || true"
	cleanWs(patterns: [[pattern: 'download_cache/', type: 'EXCLUDE']],
	        deleteDirs: true)
}

pipeline {
	agent none

	options {
		timestamps()
		skipDefaultCheckout()
	}

	parameters {
		string(name: 'BUILD_CACHE_NAME',
			   defaultValue: 'init_from_2021-10-22_1',
			   description: 'Which buildcache to use? They reside under $HOME/build_caches/$BUILD_CACHE_NAME and will be created if they do not exist.')
	}

	environment {
		CONTAINER_STYLE = "visionary"
	}

	stages {
		stage('Container Build') {
			agent { label 'conviz1||conviz2' }
			environment {
				DOCKER_BASE_IMAGE = "debian:bullseye"
				// FIXME: ^${DEPENDENCY_PYTHON} is a workaround for an invalid spectrum-mpi concretization
				DEPENDENCY_PYTHON = "python@2.7.18"
				// This needs to be here because otherwise the default python
				// (2.7.18) will pollute the spec and lead to a conflict
				// can be removed as soon as the explicit preferred version
				// is dropped
				DEPENDENCY_PYTHON3 = "python@3.8.2"
				VISIONARY_GCC_VERSION = "11.2.0"
				VISIONARY_GCC = "gcc@${VISIONARY_GCC_VERSION}"
				TMPDIR = "/tmp/${env.NODE_NAME}"
				JOB_TMP_SPACK = sh(script: "mkdir -p ${env.TMPDIR} &>/dev/null; mktemp -d ${env.TMPDIR}/spack-XXXXXXXXXX",
				                   returnStdout: true).trim()
				BUILD_CACHE_NAME = "${params.BUILD_CACHE_NAME}"  // propagate parameter to environment
			}
			stages {
				stage('Pre-build Cleanup') {
					steps {
						script {
							cleanupSteps()
						}
						sh "mkdir -p \"${env.JOB_TMP_SPACK}\" && chmod 777 \"${env.JOB_TMP_SPACK}\""
					}
				}
				stage('yashchiki Checkout') {
					steps {
						checkout scm
					}
				}
				stage('Validate environment') {
					steps {
						sh ".ci/validate_environment.sh"
					}
				}
				stage('Spack Clone') {
					steps {
						sh ".ci/clone.sh"
					}
				}
				stage('Dump Meta Info') {
					steps {
						sh ".ci/dump_meta_info.sh"
						sh ".ci/notify_gerrit.sh -m 'Build containing this change started..'"
					}
				}
				stage('Spack Fetch') {
					steps {
						script {
							try {
								sh ".ci/fetch.sh"
							}
							catch (Throwable t) {
								archiveArtifacts "errors_concretization.log"
								throw t
							}
							spec_folder_in_container = sh(script: ".ci/get_jenkins_env.sh SPEC_FOLDER_IN_CONTAINER", returnStdout: true).trim()
							archiveArtifacts(artifacts: "sandboxes/*/$spec_folder_in_container/*.yaml", allowEmptyArchive: true)
						}
					}
				}
				stage('Deploy utilities') {
					steps {
						sh ".ci/deploy_utilities.sh"
					}
				}
				stage('Create visionary recipe') {
					steps {
						sh ".ci/visionary_create_recipe.sh"
					}
				}
				stage('Build sandbox') {
					steps {
						sh ".ci/build_sandbox.sh"
					}
				}
				stage('Build container image') {
					steps {
						sh ".ci/build_image.sh"
					}
				}
				stage('Update build cache and export container') {
					steps {
						script {
							// we only want the container name, tail everything else
							CONTAINER_IMAGE = sh(script: ".ci/deploy_container.sh | tail -n 1", returnStdout: true).trim()
						}
						sh ".ci/update_build_cache.sh -c \"$CONTAINER_IMAGE\""
						sh ".ci/notify_gerrit.sh -t Build -c \"$CONTAINER_IMAGE\""
					}
				}
			}
			post {
				failure {
					script {
						cache_failed = sh(script: ".ci/create_temporary_build_cache_after_failure.sh", returnStdout: true).trim()
					}
					sh ".ci/notify_gerrit.sh -v -1 -t Build -m \"Successfully built packages stored in cache. Resume by issuing:\nWITH_CACHE_NAME=${cache_failed}\n\nIn your next gerrit comment, NOT commit message!\""
				}
				cleanup {
					archiveArtifacts "jenkins.env"
					archiveArtifacts "out_singularity_build_visionary_recipe.txt"
					// Clean build artifacts because otherwise the latest build from each jenkins job can take up to 50GB.
					// 2 executors and 5 Jenkins-Jobs (testing, testing-spack, testing-asic, stable, stable-asic) will slowly but surely eat away memory.
					script {
						cleanupSteps()
					}
				}
			}
		}

		// Container verification stage: Build visionary metaprojects
		stage('Container Verification') {
			parallel {

				// BSS1
				stage('NMPM Software') {
					steps {
						build(job: 'bld_gerrit-meta-nmpm-software',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}

				// BSS2
				stage('PPU Toolchain') {
					steps {
						build(job: 'bld_gerrit-ppu-toolchain-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('haldls') {
					steps {
						build(job: 'bld_gerrit-haldls-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('hxtorch') {
					steps {
						build(job: 'bld_gerrit-hxtorch-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('pynn-brainscales') {
					steps {
						build(job: 'bld_gerrit-pynn-brainscales-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('documentation-brainscales2') {
					steps {
						build(job: 'doc_gerrit_documentation-brainscales2-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}

				// HDBIOAI project code
				stage('HDBIOAI project') {
					steps {
						build(job: 'bld_gerrit-model-hw-hdbioai-dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				// neuromorphic sampling library
				stage('model-nmsampling-sbs') {
					steps {
						build(job: 'bld_gerrit_model-nmsampling-sbs_dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				// Visionary KiCad library
				stage('pcb-kicad-lib') {
					steps {
						build(job: 'bld_gerrit_pcb-kicad-lib_dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				// Visionary lab tools
				stage('labcontrol') {
					steps {
						build(job: 'bld_gerrit_labcontrol_dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
			}
			post {
				success {
					node('conviz1||conviz2') {
						// singularityArgs needed because conviz is still running singularity 2.6 due to faster image build times
						inSingularity(image: CONTAINER_IMAGE, singularityArgs: "-B /etc/passwd") {
							jesh "/opt/spack_install_scripts/notify_gerrit.sh -v 1 -t Tests -c '${CONTAINER_IMAGE}'"
						}
					}
				}
				unstable {
					node ('conviz1||conviz2') {
						// singularityArgs needed because conviz is still running singularity 2.6 due to faster image build times
						inSingularity(image: CONTAINER_IMAGE, singularityArgs: "-B /etc/passwd") {
							jesh "/opt/spack_install_scripts/notify_gerrit.sh -v 0 -t Tests -c '${CONTAINER_IMAGE}'"
						}
					}
				}
				failure {
					node ('conviz1||conviz2') {
						// singularityArgs needed because conviz is still running singularity 2.6 due to faster image build times
						inSingularity(image: CONTAINER_IMAGE, singularityArgs: "-B /etc/passwd") {
							jesh "/opt/spack_install_scripts/notify_gerrit.sh -v -1 -t Tests -c '${CONTAINER_IMAGE}'"
						}
					}
				}
			}
		}
	}
	post {
		failure {
			notifyFailure(mattermostChannel: "#spack")
		}
	}
}
