@Library("jenlib") _

Closure cleanupSteps = {
	// NOTE: sudo commands have been manually permitted
	// remove sandboxes
	sh "sudo /bin/rm -rf \"${WORKSPACE}/sandboxes/\" || exit 0"
	// remove tmp spack
	sh "sudo /bin/rm -rf \"/tmp/${NODE_NAME}/\""
	// the spack repository gets bind mounted into the sandbox and owned by
	// spack user during build -> revert prior to cleaning worksapce
	sh "[ -d \"$WORKSPACE/spack\" ] && sudo chown -R vis_jenkins \"$WORKSPACE/spack\" || true"
	cleanWs(patterns: [[pattern: 'download_cache/', type: 'EXCLUDE']],
	        deleteDirs: true)
}

pipeline {
	agent { label 'conviz1||conviz2' }

	options {
		timestamps()
		skipDefaultCheckout()
	}

	parameters {
		string(name: 'BUILD_CACHE_NAME',
			   defaultValue: 'init_from_2022-06-21_1',
			   description: 'Which buildcache to use? They reside under $HOME/build_caches/$BUILD_CACHE_NAME and will be created if they do not exist.')
	}

	environment {
		CONTAINER_STYLE = "visionary"
	}

	stages {
		stage('Container Build') {
			environment {
				DOCKER_BASE_IMAGE = "debian:bullseye"
				// FIXME: ^${DEPENDENCY_PYTHON} is a workaround for an invalid spectrum-mpi concretization
				DEPENDENCY_PYTHON = "python@2.7.18"
				// This needs to be here because otherwise the default python
				// (2.7.18) will pollute the spec and lead to a conflict
				// can be removed as soon as the explicit preferred version
				// is dropped
				DEPENDENCY_PYTHON3 = "python@3.8.2"
				YASHCHIKI_PROXY_HTTP = "http://proxy.kip.uni-heidelberg.de:8080"
				YASHCHIKI_PROXY_HTTPS = "http://proxy.kip.uni-heidelberg.de:8080"
				YASHCHIKI_BUILD_SPACK_GCC = 1
				YASHCHIKI_SPACK_GCC_VERSION = "11.2.0"
				YASHCHIKI_SPACK_GCC = "gcc@${YASHCHIKI_SPACK_GCC_VERSION}"
				TMPDIR = "/tmp/${env.NODE_NAME}"
				JOB_TMP_SPACK = sh(script: "mkdir -p ${env.TMPDIR} &>/dev/null; mktemp -d ${env.TMPDIR}/spack-XXXXXXXXXX",
				                   returnStdout: true).trim()
				BUILD_CACHE_NAME = "${params.BUILD_CACHE_NAME}"  // propagate parameter to environment
			}
			stages {
				stage('Pre-build Cleanup') {
					steps {
						script {
							cleanupSteps()
						}
						sh "mkdir -p \"${env.JOB_TMP_SPACK}\" && chmod 777 \"${env.JOB_TMP_SPACK}\""
					}
				}
				stage('yashchiki Checkout') {
					steps {
						script {
							sh "git clone ssh://hudson@brainscales-r.kip.uni-heidelberg.de:29418/waf.git symwaf2ic"
							sh "cd symwaf2ic && singularity exec /containers/stable/latest make"
							if (!env.GERRIT_CHANGE_NUMBER) {
								sh "singularity exec /containers/stable/latest ./symwaf2ic/waf setup --project=yashchiki --clone-depth=2"
							} else {
								sh "singularity exec /containers/stable/latest ./symwaf2ic/waf setup --project=yashchiki --clone-depth=2 --gerrit-changes=${GERRIT_CHANGE_NUMBER} --gerrit-url=ssh://hudson@${GERRIT_HOST}:${GERRIT_PORT}"
							}
						}
					}
				}
				stage('Validate environment') {
					steps {
						sh "yashchiki/.ci/validate_environment.sh"
					}
				}
				stage('Dump Meta Info') {
					steps {
						sh "yashchiki/.ci/dump_meta_info.sh"
						sh "yashchiki/.ci/notify_gerrit.sh -m 'Build containing this change started..'"
					}
				}
				stage('Spack Fetch') {
					steps {
						script {
							try {
								sh "yashchiki/.ci/fetch.sh"
							}
							catch (Throwable t) {
								archiveArtifacts "errors_concretization.log"
								throw t
							}
							spec_folder_in_container = sh(script: "yashchiki/.ci/get_host_env.sh SPEC_FOLDER_IN_CONTAINER", returnStdout: true).trim()
							archiveArtifacts(artifacts: "sandboxes/*/$spec_folder_in_container/*.yaml", allowEmptyArchive: true)
						}
					}
				}
				stage('Deploy utilities') {
					steps {
						sh "yashchiki/.ci/deploy_utilities.sh"
					}
				}
				stage('Create visionary recipe') {
					steps {
						sh "yashchiki/.ci/visionary_create_recipe.sh"
					}
				}
				stage('Build sandbox') {
					steps {
						sh "yashchiki/.ci/build_sandbox.sh"
					}
				}
				stage('Build container image') {
					steps {
						sh "yashchiki/.ci/build_image.sh"
					}
				}
				stage('Update build cache and export container') {
					steps {
						script {
							// we only want the container name, tail everything else
							CONTAINER_IMAGE = sh(script: "yashchiki/.ci/deploy_container.sh | tail -n 1", returnStdout: true).trim()
						}
						sh "yashchiki/.ci/update_build_cache.sh -c \"$CONTAINER_IMAGE\""
						sh "yashchiki/.ci/notify_gerrit.sh -t Build -c \"$CONTAINER_IMAGE\""
					}
				}
			}
			post {
				failure {
					script {
						cache_failed = sh(script: "yashchiki/.ci/create_temporary_build_cache_after_failure.sh", returnStdout: true).trim()
					}
					sh "yashchiki/.ci/notify_gerrit.sh -v -1 -t Build -m \"Successfully built packages stored in cache. Resume by issuing:\nWITH_CACHE_NAME=${cache_failed}\n\nIn your next gerrit comment, NOT commit message!\""
				}
				cleanup {
					archiveArtifacts "host.env"
					archiveArtifacts "out_singularity_build_visionary_recipe.txt"
				}
			}
		}

		// Container verification stage: Build visionary metaprojects
		stage('Container Verification') {
			parallel {

				// BSS1
				stage('NMPM Software') {
					steps {
						build(job: 'bld_gerrit-meta-nmpm-software',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}

				// BSS2
				stage('PPU Toolchain') {
					steps {
						build(job: 'bld_gerrit-ppu-toolchain-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('haldls') {
					steps {
						build(job: 'bld_gerrit-haldls-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('hxtorch') {
					steps {
						build(job: 'bld_gerrit-hxtorch-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('pynn-brainscales') {
					steps {
						build(job: 'bld_gerrit-pynn-brainscales-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				stage('documentation-brainscales2') {
					steps {
						build(job: 'doc_gerrit_documentation-brainscales2-dependencies',
						      parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}

				// neuromorphic sampling library
				stage('model-nmsampling-sbs') {
					steps {
						build(job: 'bld_gerrit_model-nmsampling-sbs_dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				// Visionary KiCad library
				stage('pcb-kicad-lib') {
					steps {
						build(job: 'bld_gerrit_pcb-kicad-lib_dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
				// Visionary lab tools
				stage('labcontrol') {
					steps {
						build(job: 'bld_gerrit_labcontrol_dependencies',
							  parameters: [string(name: 'OVERWRITE_DEFAULT_CONTAINER_IMAGE', value: CONTAINER_IMAGE)])
					}
				}
			}
			post {
				success {
					// singularityArgs needed because conviz is still running singularity 2.6 due to faster image build times
					inSingularity(image: CONTAINER_IMAGE, singularityArgs: "-B /etc/passwd") {
						jesh "/opt/spack_install_scripts/notify_gerrit.sh -v 1 -t Tests -c '${CONTAINER_IMAGE}'"
					}
				}
				unstable {
					// singularityArgs needed because conviz is still running singularity 2.6 due to faster image build times
					inSingularity(image: CONTAINER_IMAGE, singularityArgs: "-B /etc/passwd") {
						jesh "/opt/spack_install_scripts/notify_gerrit.sh -v 0 -t Tests -c '${CONTAINER_IMAGE}'"
					}
				}
				failure {
					// singularityArgs needed because conviz is still running singularity 2.6 due to faster image build times
					inSingularity(image: CONTAINER_IMAGE, singularityArgs: "-B /etc/passwd") {
						jesh "/opt/spack_install_scripts/notify_gerrit.sh -v -1 -t Tests -c '${CONTAINER_IMAGE}'"
					}
				}
			}
		}
	}
	post {
		failure {
			notifyFailure(mattermostChannel: "#spack")
		}
		cleanup {
			// Clean build artifacts because otherwise the latest build from each jenkins job can take up to 50GB.
			// 2 executors and 5 Jenkins-Jobs (testing, testing-spack, testing-asic, stable, stable-asic) will slowly but surely eat away memory.
			script {
				cleanupSteps()
			}
		}
	}
}
